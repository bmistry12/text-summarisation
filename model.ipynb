{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "colab_type": "code",
    "id": "bV27LCvjfwkR",
    "outputId": "a04607a8-4104-4fac-f7a9-bcf188644af3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "# !python -m nltk.downloader stopwords wordnet punkt averaged_perceptron_tagger\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.layers import Attention\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "# !pip install rouge\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2feqtBEHiaS4"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omclwb_CiEZr"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=40\n",
    "EPOCHS=30\n",
    "latent_dim=128\n",
    "embedding_dim=128\n",
    "test_train_split=0.15\n",
    "build_number=\"1\"\n",
    "LEARNING_RATE=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rouge object for evaluation\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNPOLXNLiiX5"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cchT-sW8kGP4"
   },
   "source": [
    "Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "1LLVyg34iLP5",
    "outputId": "5c7f9c18-8095-4ea8-a1ed-96d81e0494a1"
   },
   "outputs": [],
   "source": [
    "# Only needed if running on Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "colab_type": "code",
    "id": "Y3Op7xyUiHSv",
    "outputId": "4ede3aaf-b804-4c17-ec67-da51ca0ab2c3"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./drive/My Drive/originals-l.csv')\n",
    "df = pd.read_csv('./originals-l.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "ptOkktH2CLqg",
    "outputId": "b18d889a-8154-4e55-bb8c-c8777b968dfd"
   },
   "outputs": [],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ogB1RqsqreG2"
   },
   "source": [
    "Split for now so we are only aiming for one summary per text (This is now in dataprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['summary'][0])\n",
    "df['summary'] = df['summary'].apply(lambda x: re.sub(r'\\..*$',' ',str(x)))\n",
    "print(df['summary'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZzZjP4vFzVw"
   },
   "source": [
    "Remove .'s that appear in stuff like U.S.A and U.N - Eventually need to move this to dataprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "60CqA1Km6PvJ",
    "outputId": "97d25b7d-2550-4b2b-922c-dbdb103f520b"
   },
   "outputs": [],
   "source": [
    "print(df['summary'][0])\n",
    "df['summary'] = df['summary'].apply(lambda x: re.sub(r'\\.','',str(x)))\n",
    "print(df['summary'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmYf4I1bGA8N"
   },
   "source": [
    "Check for rows with null values in them, and copy these into a new dataframe (df1). Drop any rows in df1 from df to ensure no NaN valued rows are present/\n",
    "\n",
    "*Note. using simply dropna(how='any') does not seem to drop any of the rows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "5Ws0uSZv8Xnw",
    "outputId": "1fd60887-8a8d-45f4-8058-461451510fdd"
   },
   "outputs": [],
   "source": [
    "print(df.isnull().values.any())\n",
    "print(df.shape)\n",
    "\n",
    "df1 = df[df.isna().any(axis=1)]\n",
    "print(df1.shape)\n",
    "\n",
    "df.drop(df1.index, axis=0,inplace=True)\n",
    "print(df.shape)\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSVW1P-Ji_7R"
   },
   "source": [
    "Word Count Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "M72pGpsDjCrc",
    "outputId": "a0a964c6-2199-4af9-bcca-483858455fbf"
   },
   "outputs": [],
   "source": [
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in df['text']:\n",
    "      text_word_count.append(len(i.split(' ')))\n",
    "\n",
    "for i in df['summary']:\n",
    "      summary_word_count.append(len(i.split(' ')))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.ylabel('Documents')\n",
    "plt.xlabel('Word Count')\n",
    "plt.savefig('word_count_distro' + str(build_number) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNZ2tVpljHTc"
   },
   "source": [
    "Max Text Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "3T01Jf-XjGKO",
    "outputId": "9391bff2-c725-4c04-be4b-1787052d38a8"
   },
   "outputs": [],
   "source": [
    "max_text_len = max([len(txt) for txt in df['text']])\n",
    "max_summary_len = max([len(txt) for txt in df['summary']])\n",
    "print(max_text_len)\n",
    "print(max_summary_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGl1z0OFjTsr"
   },
   "source": [
    "### Training-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SwmBST04juu6"
   },
   "source": [
    "X - Articles text </br>\n",
    "Y - Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1_YrHcDjN6e"
   },
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "X = np.array(df['text'])\n",
    "Y = np.array(df['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "XGHJXwaojYAV",
    "outputId": "b7e01904-a40c-48aa-c9dd-e8edbdc72303"
   },
   "outputs": [],
   "source": [
    "x_tr,x_val,y_tr,y_val=train_test_split(X,Y,test_size=test_train_split,random_state=0,shuffle=True)\n",
    "print(x_tr.shape)\n",
    "print(x_val.shape)\n",
    "print(y_tr.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzWpoc9OjjdV"
   },
   "source": [
    "### Word Embeddings - Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLVdVklnjq1i"
   },
   "source": [
    "X Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7qUrNpbjpI2"
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "text = df['text']\n",
    "\n",
    "for row in text: \n",
    "  for word in row.split(\" \"):\n",
    "    if word not in word_dict:\n",
    "      word_dict[word] = 1\n",
    "    else:\n",
    "      word_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "6qbg_6TKj4HK",
    "outputId": "87fc2439-5039-480b-a9d3-0a23b4020b78"
   },
   "outputs": [],
   "source": [
    "# #prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=len(word_dict)) \n",
    "x_tokenizer.fit_on_texts(list(X))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1\n",
    "print(x_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdwGLWiRloV9"
   },
   "outputs": [],
   "source": [
    "with open('xtokenizer.pickle', 'wb') as handle:\n",
    "  pickle.dump(x_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NsdijM3j6RJ"
   },
   "source": [
    "Y Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CG6Q2G-Wj79A"
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "text = df['summary']\n",
    "\n",
    "for row in text: \n",
    "  for word in row.split(\" \"):\n",
    "    if word not in word_dict:\n",
    "      word_dict[word] = 1\n",
    "    else:\n",
    "      word_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "C5ugAAnIj91g",
    "outputId": "7dba3458-13b0-4c88-f300-e120ac7a024d"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=len(word_dict)) \n",
    "y_tokenizer.fit_on_texts(list(Y))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1\n",
    "print(y_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzpeT5fXlp3-"
   },
   "outputs": [],
   "source": [
    "with open('ytokenizer.pickle', 'wb') as handle:\n",
    "  pickle.dump(y_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zntqptcwj_lh"
   },
   "source": [
    "## Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uc0JOUvqkWUL"
   },
   "source": [
    "#### Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "id": "9dx-9BT6kZMw",
    "outputId": "d63bb2a5-6f96-475a-ed30-62bc910b40f9"
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc,embedding_dim,trainable=True)(encoder_inputs)\n",
    "#encoder lstm \n",
    "encoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JJUPBdfkdlb"
   },
   "source": [
    "#### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLgYfdF3kb1A"
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "                                                          \n",
    "#dense layer\n",
    "decoder_dense = Dense(y_voc, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "ASIlTOT_khrn",
    "outputId": "2040df81-aa4a-46d0-d333-3ca27ae0e22a"
   },
   "outputs": [],
   "source": [
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=LEARNING_RATE, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "gMQ1ZrF0ksMT",
    "outputId": "b9e0f087-6ffe-4e9c-bf9a-9cd44cb5183a"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o12XRjJykuA-"
   },
   "source": [
    "Early Stopping Callback to ensure we stop when Validation Loss is lowest - minimises risk of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6R-qwZoNkspo"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2, restore_best_weights=False)\n",
    "filepath = \"./drive/My Drive/project-model/saved-model-{epoch:02d}.hdf5\"\n",
    "mc = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this method to train a new model. To continue training a previously trained model see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "colab_type": "code",
    "id": "Pbnxf9ZWk4I0",
    "outputId": "75fa86fc-41d5-40f5-b8be-adc5e841ea69"
   },
   "outputs": [],
   "source": [
    "history = model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:], batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[es], validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method is to only be used when loading a previously partially trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_num = '05'\n",
    "# model = load_model(\"./drive/My Drive/project-model/saved-model-\" + epoch_num + \".hdf5\")\n",
    "# history = model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:], callbacks=[mc], batch_size=BATCH_SIZE, epochs=1, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEkRn71fk7up"
   },
   "outputs": [],
   "source": [
    "model.save('model' + str(build_number) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "N52zaYcKlAXE",
    "outputId": "ecb37c11-d7b1-4dd6-b2f6-818668982686"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('loss' + str(build_number) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxSFAxiWlDad"
   },
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cm1b46qtlFnT"
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1Q7SWH_lGxE"
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_inputs)\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c], [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "biBjzpLQlPhf",
    "outputId": "2860850a-d076-4102-9f76-e0f1b63899bf"
   },
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "wmvKd7pNlQXs",
    "outputId": "ae6d08b1-ccca-421d-9597-40e81cb9ea2d"
   },
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywir6L6wlUE-"
   },
   "source": [
    "### Methhods for Reversing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoKuk0dElZZm"
   },
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSR-0MHclb0G"
   },
   "source": [
    "### Summarisation Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7WYZgAXelbLU"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq): \n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, y_voc))\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "      output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c]) \n",
    "      # print(output_tokens[0, -1, :][1:])\n",
    "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "      # print(sampled_token_index)\n",
    "      # print(sampled_token_index)\n",
    "      if (sampled_token_index != 0 ):\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_token\n",
    "      else :\n",
    "        stop_condition = True\n",
    "      if (len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "              stop_condition = True\n",
    "       # Update the target sequence (of length 1).\n",
    "      # target_seq = np.zeros((1,1))\n",
    "      target_seq = np.zeros((1, y_voc))\n",
    "      target_seq[0, sampled_token_index] = 1\n",
    "\n",
    "      # Update internal states\n",
    "      e_h, e_c = h, c\n",
    "      \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STnQkiZzlhbb"
   },
   "source": [
    "## Test Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AisRsa573_L3"
   },
   "source": [
    "Note: *I think there isn't enough data being passed in and so the argmax value always is 0 - it can't learn what should be next*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRouge(gt, pred):\n",
    "  return rouge.get_scores(pred, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "IC1oCOOHlgjh",
    "outputId": "c93bf6f0-f462-4ff3-b0c2-9ac79671518a"
   },
   "outputs": [],
   "source": [
    "for i in range(0,1):\n",
    "    print(\"Article:\",seq2text(x_tr[i]))\n",
    "    original = seq2summary(y_tr[i])\n",
    "    print(\"Original summary:\",original)\n",
    "    summary = decode_sequence(x_tr)\n",
    "    print(\"Generated summary:\",summary)\n",
    "    print(\"\\n\")\n",
    "    print(\"ROUGE score: \")\n",
    "    score = getRouge(str(summary), str(original))\n",
    "    print(score)\n",
    "    print(score[0].get('rouge-1').get('f'))\n",
    "    print(score[0].get('rouge-1').get('p'))\n",
    "    print(score[0].get('rouge-1').get('r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2479,2480):\n",
    "    print(\"Article:\",seq2text(x_tr[i]))\n",
    "    original = seq2summary(y_tr[i])\n",
    "    print(\"Original summary:\",original)\n",
    "    summary = decode_sequence(x_tr)\n",
    "    print(\"Generated summary:\",summary)\n",
    "    print(\"\\n\")\n",
    "    print(\"ROUGE score: \")\n",
    "    score = getRouge(str(summary), str(original))\n",
    "    print(score)\n",
    "    print(score[0].get('rouge-1').get('f'))\n",
    "    print(score[0].get('rouge-1').get('p'))\n",
    "    print(score[0].get('rouge-1').get('r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNy_KJPb4PXc"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMRIXeuz8hYn"
   },
   "source": [
    "Using ROUGE (Recall-Orientated Understanding Gisting Evaluation) to evaluate the generated summaries\n",
    "\n",
    "*Note: This takes a long time, especially with large datasets* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqM1FoWQFsqJ"
   },
   "outputs": [],
   "source": [
    "def get_overlapping_words(x, y):\n",
    "  num=0\n",
    "  x = nltk.word_tokenize(x)\n",
    "  y = nltk.word_tokenize(y)\n",
    "  for word in y:\n",
    "    if word in x:\n",
    "      num = num+1\n",
    "      x.remove(word)\n",
    "    else:\n",
    "      return num\n",
    "\n",
    "def precision(target, generated):\n",
    "  length = len(target)\n",
    "  for i in range (0, length):\n",
    "    num_overlapping_words = get_overlapping_words(target[i], generated[i])\n",
    "    generated_summary_len = len(generated[i])\n",
    "    if generated_summary_len == 0 :\n",
    "        return 0.0\n",
    "    else : \n",
    "      return num_overlapping_words / generated_summary_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwTD6W5kFmvk"
   },
   "source": [
    "### For Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "OUr2YcMoDGEt",
    "outputId": "975bec9a-bdd2-4ead-aa41-9a8eb4bb44bd"
   },
   "outputs": [],
   "source": [
    "print(len(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "-BKXHKbaDS8G",
    "outputId": "e57e5de1-e341-4d9e-d5ff-eeee0baa7d34"
   },
   "outputs": [],
   "source": [
    "tr_target_summary = []\n",
    "tr_generated_summary = []\n",
    "x_tr_len = len(x_tr)\n",
    "\n",
    "f_ov = 0\n",
    "p_ov = 0\n",
    "r_ov = 0\n",
    "# x_val_len = 1\n",
    "for i in range(0,x_tr_len):\n",
    "  original = seq2summary(y_tr[i])\n",
    "  tr_target_summary.append(original)\n",
    "  x_i = x_tr[i].reshape(1,max_text_len)\n",
    "  summary = decode_sequence(x_i)\n",
    "  tr_generated_summary.append(summary)\n",
    "  score = getRouge(str(summary), str(original))\n",
    "  f_ov += float(score[0].get('rouge-1').get('f'))\n",
    "  p_ov += float(score[0].get('rouge-1').get('p'))\n",
    "  r_ov += float(score[0].get('rouge-1').get('r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "DuzxQo3E4SlK",
    "outputId": "2f3d240c-5169-4bef-8800-3c85b2530cfe"
   },
   "outputs": [],
   "source": [
    "# print(\"precision : \" + str(precision(tr_target_summary, tr_generated_summary)))\n",
    "print(\"Avg F Score: \" + str(f_ov/x_tr_len))\n",
    "print(\"Avg Precision: \" + str(p_ov/x_tr_len))\n",
    "print(\"Avg Recall: \" + str(r_ov/x_tr_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KiHBkkwmFTPt"
   },
   "source": [
    "### For Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "B2QGuitHFZD1",
    "outputId": "5d8559aa-68b2-4492-c879-57b5a2e2e1ae"
   },
   "outputs": [],
   "source": [
    "val_target_summary = []\n",
    "val_generated_summary = []\n",
    "x_val_len = len(x_val)\n",
    "f_ov = 0\n",
    "p_ov = 0\n",
    "r_ov = 0\n",
    "\n",
    "for i in range(0,x_val_len):\n",
    "  original = seq2summary(y_val[i])\n",
    "  val_target_summary.append(original)\n",
    "  x_i = x_val[i].reshape(1,max_text_len)\n",
    "  summary = decode_sequence(x_i)\n",
    "  val_generated_summary.append(summary)\n",
    "  score = getRouge(str(summary), str(original))\n",
    "  f_ov += float(score[0].get('rouge-1').get('f'))\n",
    "  p_ov += float(score[0].get('rouge-1').get('p'))\n",
    "  r_ov += float(score[0].get('rouge-1').get('r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "S9n0nDG-F6KN",
    "outputId": "b31a3a45-9b75-410a-83c5-f75a3cea0dfe"
   },
   "outputs": [],
   "source": [
    "# print(\"precision : \" + str(precision(val_target_summary, val_generated_summary)))\n",
    "print(\"Avg F Score: \" + str(f_ov/x_val_len))\n",
    "print(\"Avg Precision: \" + str(p_ov/x_val_len))\n",
    "print(\"Avg Recall: \" + str(r_ov/x_val_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "faHKW5pUF9KH"
   },
   "source": [
    "# Inputting New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "srBPE2ZxZH72"
   },
   "outputs": [],
   "source": [
    "def getpos(word):\n",
    "  pos = nltk.pos_tag([word])[0][1][0]\n",
    "  wordnet_conv = {\"J\": wn.ADJ, \"N\": wn.NOUN, \"V\": wn.VERB, \"R\": wn.ADV}\n",
    "  if pos in wordnet_conv.keys():\n",
    "    return wordnet_conv.get(pos)\n",
    "  return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5aEv4W8ZM81"
   },
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  text_tokenized = inp_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "  print(\"lemmatize with pos\")\n",
    "  for i in range(0,len(text_tokenized)):\n",
    "    text_lemmatized = []\n",
    "    for word in text_tokenized[i]:\n",
    "      pos = getpos(word)\n",
    "      if pos != \"\":\n",
    "        lemma = lemmatizer.lemmatize(word, pos)\n",
    "        text_lemmatized.append(lemma)\n",
    "      else :\n",
    "        text_lemmatized.append(word)\n",
    "    text_lemmatized = ' '.join(map(str, text_lemmatized))\n",
    "    inp_df['text'][i] = text_lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "colab_type": "code",
    "id": "5tlZ-4XXuctn",
    "outputId": "06fd2777-a9d5-4479-c8a6-3fbf3bc7d9f7"
   },
   "outputs": [],
   "source": [
    "input1 = \"(CNN) — Earlier this year, Delta Air Lines announced a rethink on reclining seats. In an effort to disrupt fewer passengers' travel experiences, Delta said it'd begin revamping some of its jets to reduce the recline of coach seats from four inches to two inches and the recline of first class seats from 5.5 inches to 3.5 inches. For those who abhor the recline option, it's a small step. And for those who value it, well, it's a compromise. This seemingly innocuous topic is one where there are very much two minds on what's acceptable and what's not. Two CNN Travel staffers engage in a friendly debate about seat recline. Your seat. Your decision. Stacey Lastoe, senior editor at CNN Travel, is of above-average height and makes no apology about reclining; it's her right as a plane, train and bus passenger. She encourages the person sitting in front of her to recline as well. On the first leg of my flight to Japan for my honeymoon, my husband and I got upgraded to first class. Although it would just be a few hours in the sky en route to Dallas, I was excited about sipping Champagne, sitting back and relaxing. Flute in hand, I pushed back to recline my seat for maximum relaxation. But it would not budge; I appeared to be stuck in a dysfunctional seat. Or was I? Turns out the gentleman behind me had a dog in a crate down between his legs, positioned so the seat in front of his -- my seat -- had nowhere to go. Because we were newlyweds and loving every moment of it, I did not mind when my husband turned to the man and told him his wife wanted to recline her seat and asked if he could please rearrange his dog crate to allow for everyones comfort.\"\n",
    "inp_df = pd.DataFrame(columns=['text', 'summary'])\n",
    "inp_df = inp_df.append({'text': str(input1), 'summary': \"\"}, ignore_index=True)\n",
    "inp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OyDVNd3pvxIX"
   },
   "outputs": [],
   "source": [
    "inp_df['text'] = inp_df['text'].apply(lambda x: re.sub(r'\\(CNN\\)|--|[^\\w\\s\\.]','',x)).apply(lambda x: re.sub(r'(\\.(?=[\\s\\r\\n]|$))','',x)).apply(lambda x: re.sub(r'\\n',' ',x)).apply(lambda x: re.sub(r'\\.','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WyRVL-DGACF"
   },
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "inp_df['text'] = inp_df['text'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: \" \".join([word for word in x if not word in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "Nd2C6wFAxU3K",
    "outputId": "b376cba2-7e6d-4bec-bb18-4728cfe5e1d8"
   },
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "lemmatization(inp_df['text'])\n",
    "print(inp_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "vwj2UIuQ20Hl",
    "outputId": "635efab9-66ef-4d2b-ec07-a69b07d0ed39"
   },
   "outputs": [],
   "source": [
    "seq = np.array(inp_df['text'])\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Xx7fUFkZ0aAr",
    "outputId": "209a9b33-800e-43c9-bfcb-8e7de055e165"
   },
   "outputs": [],
   "source": [
    "seq_tokenizer = x_tokenizer.texts_to_sequences(seq)\n",
    "#padding zero upto maximum length\n",
    "seq_tokenizer_padded = pad_sequences(seq_tokenizer,  maxlen=max_text_len, padding='post')\n",
    "\n",
    "gen_summary = decode_sequence(seq_tokenizer_padded)\n",
    "\n",
    "original_txt = ' '.join(seq)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Original: \" + original_txt)\n",
    "print(\"Generated Summary: \" + gen_summary)\n",
    "print(\"ROUGE score: \")\n",
    "print(getRouge(summary, original_txt))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
