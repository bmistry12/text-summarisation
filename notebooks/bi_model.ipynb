{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bV27LCvjfwkR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "!python -m nltk.downloader stopwords wordnet punkt averaged_perceptron_tagger # remove if not running on colab\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.layers import Attention\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "!pip install rouge # remove if not running on colab\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2feqtBEHiaS4"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omclwb_CiEZr"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=40\n",
    "EPOCHS=50\n",
    "LATENT_DIM=256\n",
    "EMBEDDING_DIM=128\n",
    "TEST_TRAIN_SPLIT=0.15\n",
    "LEARNING_RATE=0.005\n",
    "MAX_TEXT_LEN = 250\n",
    "MAX_SUMMARY_LEN = 20\n",
    "UNCOMMON_WORD_THRESHOLD = 15\n",
    "COLAB=True # true if running on colab\n",
    "build_number=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0L1hVy0J07Gk"
   },
   "outputs": [],
   "source": [
    "# create rouge object for evaluation\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNPOLXNLiiX5"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cchT-sW8kGP4"
   },
   "source": [
    "Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1LLVyg34iLP5"
   },
   "outputs": [],
   "source": [
    "# Only needed if running on Google Colab\n",
    "if COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  PATH = \"./drive/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3Op7xyUiHSv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH + FILE_NAME)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptOkktH2CLqg"
   },
   "outputs": [],
   "source": [
    "print(df.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZzZjP4vFzVw"
   },
   "source": [
    "Remove .'s that appear in stuff like U.S.A and U.N - Eventually need to move \n",
    "this to dataprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4e3a1UjziN5g"
   },
   "outputs": [],
   "source": [
    "print(df['summary'][0])\n",
    "df['summary'] = df['summary'].apply(lambda x: re.sub(r'\\..*$',' ',str(x)))\n",
    "print(df['summary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60CqA1Km6PvJ"
   },
   "outputs": [],
   "source": [
    "print(df['summary'][0])\n",
    "df['summary'] = df['summary'].apply(lambda x: re.sub(r'\\.','',str(x)))\n",
    "print(df['summary'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmYf4I1bGA8N"
   },
   "source": [
    "Check for rows with null values in them, and copy these into a new dataframe (df1). Drop any rows in df1 from df to ensure no NaN valued rows are present\n",
    "\n",
    "*Note. using simply dropna(how='any') does not seem to drop any of the rows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Ws0uSZv8Xnw"
   },
   "outputs": [],
   "source": [
    "print(df.isnull().values.any())\n",
    "print(df.shape)\n",
    "\n",
    "df1 = df[df.isna().any(axis=1)]\n",
    "print(df1.shape)\n",
    "\n",
    "df.drop(df1.index, axis=0,inplace=True)\n",
    "print(df.shape)\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSVW1P-Ji_7R"
   },
   "source": [
    "Word Count Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M72pGpsDjCrc"
   },
   "outputs": [],
   "source": [
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in df['text']:\n",
    "      text_word_count.append(len(i.split(' ')))\n",
    "\n",
    "for i in df['summary']:\n",
    "      summary_word_count.append(len(i.split(' ')))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.ylabel('Documents')\n",
    "plt.xlabel('Word Count')\n",
    "# plt.savefig('word_count_distro' + str(build_number) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNZ2tVpljHTc"
   },
   "source": [
    "Cut down text to MAX_TEXT_LEN words, and summaries to MAX_SUMMARY_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3T01Jf-XjGKO"
   },
   "outputs": [],
   "source": [
    "print(df['text'][0])\n",
    "df['text'] = df['text'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: \" \".join(x[:MAX_TEXT_LEN]))\n",
    "print(df['text'][0])\n",
    "\n",
    "print(df['summary'][0])\n",
    "df['summary'] = df['summary'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: \" \".join(x[:MAX_SUMMARY_LEN]))\n",
    "print(df['summary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sut_KjDz9Zdl"
   },
   "outputs": [],
   "source": [
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in df['text']:\n",
    "      text_word_count.append(len(i.split(' ')))\n",
    "\n",
    "for i in df['summary']:\n",
    "      summary_word_count.append(len(i.split(' ')))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.ylabel('Documents')\n",
    "plt.xlabel('Word Count')\n",
    "# plt.savefig('word_count_distro_word_removal' + str(build_number) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l8eVDi9VRX3W"
   },
   "source": [
    "### Finding Uncommon Words and Removing Them.\n",
    "\n",
    "Uncommon words are classified as those that occur in the whole corpus less times than UNCOMMON_WORD_THRESHOLD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPv7a92_Dr-C"
   },
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUgS5GFHRWws"
   },
   "outputs": [],
   "source": [
    "x_word_dict = {}\n",
    "text = df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "for index, row in text.iteritems():\n",
    "  for word in row:\n",
    "    if word not in x_word_dict.keys():\n",
    "      x_word_dict[word] = 1\n",
    "    else:\n",
    "      x_word_dict[word] += 1\n",
    "\n",
    "print(len(x_word_dict))\n",
    "# sort so that extracting acceptable words is more efficient for larger vocab sizes\n",
    "sorted_dict = sorted(x_word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_dict)\n",
    "x, y = zip(*sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omhetyfSRhIl"
   },
   "outputs": [],
   "source": [
    "# find words that occur more than UNCOMMON_WORD_THRESHOLD times\n",
    "accept_words = []\n",
    "for word, occ in sorted_dict:\n",
    "  if int(occ) > UNCOMMON_WORD_THRESHOLD:\n",
    "    accept_words.append(word)\n",
    "  else:\n",
    "    break\n",
    "\n",
    "# remove uncommon words    \n",
    "accept_words = [x.lower() for x in accept_words]\n",
    "print(accept_words)\n",
    "print(df['text'][2])\n",
    "df['text'] = df['text'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: \" \".join([word for word in x if word.lower() in accept_words]))\n",
    "print(df['text'][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5TWrDUm92s-A"
   },
   "outputs": [],
   "source": [
    "x_word_dict_after = {}\n",
    "text = df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "for index, row in text.iteritems():\n",
    "  for word in row:\n",
    "    if word not in x_word_dict_after.keys():\n",
    "      x_word_dict_after[word] = 1\n",
    "    else:\n",
    "      x_word_dict_after[word] += 1\n",
    "\n",
    "print(len(x_word_dict_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vR1GZt2qELZA"
   },
   "source": [
    "#### Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqxGc-SFRtjk"
   },
   "outputs": [],
   "source": [
    "y_word_dict = {}\n",
    "summary = df['summary'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "for index, row in summary.iteritems():\n",
    "  for word in row:\n",
    "    if word not in y_word_dict.keys():\n",
    "      y_word_dict[word] = 1\n",
    "    else:\n",
    "      y_word_dict[word] += 1\n",
    "\n",
    "print(len(y_word_dict))\n",
    "sorted_dict = sorted(y_word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_dict)\n",
    "x, y = zip(*sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HYzOJ8vwSKFj"
   },
   "outputs": [],
   "source": [
    "accept_words = []\n",
    "for word, occ in sorted_dict:\n",
    "  if int(occ) > UNCOMMON_WORD_THRESHOLD:\n",
    "    accept_words.append(word)\n",
    "  else:\n",
    "    break\n",
    "     \n",
    "accept_words = [x.lower() for x in accept_words]\n",
    "print(accept_words)\n",
    "print(df['summary'][2])\n",
    "df['summary'] = df['summary'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: \" \".join([word for word in x if word.lower() in accept_words]))\n",
    "print(df['summary'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7I3PjZ6Y2zey"
   },
   "outputs": [],
   "source": [
    "y_word_dict_after = {}\n",
    "summary = df['summary'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "for index, row in summary.iteritems():\n",
    "  for word in row:\n",
    "    if word not in y_word_dict_after.keys():\n",
    "      y_word_dict_after[word] = 1\n",
    "    else:\n",
    "      y_word_dict_after[word] += 1\n",
    "print(len(y_word_dict_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0KRLG8EEb64"
   },
   "source": [
    "View Word Distribution after uncommon word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1iu1XfFQSWjc"
   },
   "outputs": [],
   "source": [
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in df['text']:\n",
    "      text_word_count.append(len(i.split(' ')))\n",
    "\n",
    "for i in df['summary']:\n",
    "      summary_word_count.append(len(i.split(' ')))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.ylabel('Documents')\n",
    "plt.xlabel('Word Count')\n",
    "\n",
    "# plt.savefig('word_count_distro_word_removal' + str(build_number) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgJarGxDEj51"
   },
   "source": [
    "### Update Max Text Lenths After Uncommon Word Removal & Drop Empty Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTl9ZmZIvCPJ"
   },
   "outputs": [],
   "source": [
    "print(df['text'][0])\n",
    "df['text'] = df['text'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: \" \".join(x[:MAX_TEXT_LEN]))\n",
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxrlLdL6vDa6"
   },
   "outputs": [],
   "source": [
    "print(df['summary'][0])\n",
    "df['summary'] = df['summary'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: \" \".join(x[:MAX_SUMMARY_LEN]))\n",
    "print(df['summary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASb08ONGSbDO"
   },
   "outputs": [],
   "source": [
    "# update max text lengths\n",
    "MAX_TEXT_LEN = max([len(txt.split(' ')) for txt in df['text']])\n",
    "MAX_SUMMARY_LEN = max([len(txt.split(' ')) for txt in df['summary']])\n",
    "print(MAX_TEXT_LEN)\n",
    "print(MAX_SUMMARY_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vC4St3kG8vSe"
   },
   "outputs": [],
   "source": [
    "# drop any lines that have no text or summary after the word removal process\n",
    "print(df.isnull().values.any())\n",
    "print(df.shape)\n",
    "\n",
    "df1 = df[df.isna().any(axis=1)]\n",
    "print(df1.shape)\n",
    "\n",
    "df.drop(df1.index, axis=0,inplace=True)\n",
    "print(df.shape)\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anAO7B1K918G"
   },
   "source": [
    "### Add in start and end tokens to summaries\n",
    "This is necessary to ensure that the model can tell when each summaries start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCiui56mxGnO"
   },
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "print(df['summary'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGl1z0OFjTsr"
   },
   "source": [
    "## Training-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SwmBST04juu6"
   },
   "source": [
    "X - Articles text </br>\n",
    "Y - Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1_YrHcDjN6e"
   },
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "X = np.array(df['text'])\n",
    "Y = np.array(df['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGHJXwaojYAV"
   },
   "outputs": [],
   "source": [
    "x_tr,x_val,y_tr,y_val=train_test_split(X,Y,test_size=TEST_TRAIN_SPLIT,random_state=0,shuffle=True)\n",
    "print(x_tr.shape)\n",
    "print(x_val.shape)\n",
    "print(y_tr.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzWpoc9OjjdV"
   },
   "source": [
    "## Word Embeddings - Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLVdVklnjq1i"
   },
   "source": [
    "X Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7qUrNpbjpI2"
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "text = df['text']\n",
    "\n",
    "for row in text: \n",
    "  for word in row.split(\" \"):\n",
    "    if word not in word_dict:\n",
    "      word_dict[word] = 1\n",
    "    else:\n",
    "      word_dict[word] += 1\n",
    "\n",
    "print(len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qbg_6TKj4HK"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=len(word_dict), split=\" \") \n",
    "x_tokenizer.fit_on_texts(list(X))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=MAX_TEXT_LEN, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=MAX_TEXT_LEN, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1\n",
    "print(x_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdwGLWiRloV9"
   },
   "outputs": [],
   "source": [
    "with open('xtokenizer.pickle', 'wb') as handle:\n",
    "  pickle.dump(x_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NsdijM3j6RJ"
   },
   "source": [
    "Y Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CG6Q2G-Wj79A"
   },
   "outputs": [],
   "source": [
    "y_word_dict = {}\n",
    "summ = df['summary']\n",
    "\n",
    "for row in summ: \n",
    "  for word in row.split(\" \"):\n",
    "    if word not in y_word_dict:\n",
    "      y_word_dict[word] = 1\n",
    "    else:\n",
    "      y_word_dict[word] += 1\n",
    "\n",
    "print(len(y_word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5ugAAnIj91g"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=len(y_word_dict), split=\" \") \n",
    "y_tokenizer.fit_on_texts(list(Y))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=MAX_SUMMARY_LEN, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=MAX_SUMMARY_LEN, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words + 1\n",
    "print(y_voc)\n",
    "print(y_tokenizer.word_counts['sostok'],len(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzpeT5fXlp3-"
   },
   "outputs": [],
   "source": [
    "with open('ytokenizer.pickle', 'wb') as handle:\n",
    "  pickle.dump(y_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zntqptcwj_lh"
   },
   "source": [
    "## Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uc0JOUvqkWUL"
   },
   "source": [
    "#### Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dx-9BT6kZMw"
   },
   "outputs": [],
   "source": [
    "#encoder input\n",
    "encoder_inputs = Input(shape=(MAX_TEXT_LEN,))\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc,EMBEDDING_DIM,trainable=True)(encoder_inputs)\n",
    "# bidirectional encoder lstm \n",
    "encoder_lstm = Bidirectional(LSTM(LATENT_DIM,return_sequences=True,return_state=True))\n",
    "# outputs and encoder final states\n",
    "encoder_outputs, fw_state_h, fw_state_c, bw_state_h, bw_state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "state_h = Concatenate()([fw_state_h, bw_state_h])\n",
    "state_c = Concatenate()([fw_state_c, bw_state_c])\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JJUPBdfkdlb"
   },
   "source": [
    "#### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLgYfdF3kb1A"
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, EMBEDDING_DIM,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(LATENT_DIM*2, return_sequences=True, return_state=True)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=encoder_states)\n",
    "                                                          \n",
    "#dense layer \n",
    "#soft max creates a prob vector that allows us to determine the final output\n",
    "decoder_dense = Dense(y_voc, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rIpBF_fAkfuG"
   },
   "source": [
    "#### Combined LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASIlTOT_khrn"
   },
   "outputs": [],
   "source": [
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bi9_HXvVwS1i"
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=LEARNING_RATE, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMQ1ZrF0ksMT"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o12XRjJykuA-"
   },
   "source": [
    "- Early Stopping Callback to ensure we stop when Validation Loss is lowest - minimises risk of overfitting\n",
    "- Model Checkpoint saves the model after each epoch so that we can load the model with the best weights later on. Alternatively, it allows us to continue training the model at a later data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6R-qwZoNkspo"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2, restore_best_weights=False)\n",
    "filepath = \"./model/saved-model-{epoch:02d}.hdf5\"\n",
    "if COLAB:\n",
    "  filepath = PATH + \"project-model/saved-model-{epoch:02d}.hdf5\"\n",
    "mc = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "02-CPYWhXiYz"
   },
   "source": [
    "#### Use this  to train a new model. To continue training a previously trained model see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uK1YZYdzQJNc"
   },
   "outputs": [],
   "source": [
    "# reshape y from two dimensions to three dimensions\n",
    "y_tr_3d = y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:]\n",
    "y_val_3d = y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPxQrcwE6zmn"
   },
   "outputs": [],
   "source": [
    "print(x_tr.shape)\n",
    "print(y_tr.shape)\n",
    "print(y_tr_3d.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(y_val_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pbnxf9ZWk4I0"
   },
   "outputs": [],
   "source": [
    "history = model.fit([x_tr,y_tr[:,:-1]], y_tr_3d, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[es,mc], validation_data=([x_val,y_val[:,:-1]], y_val_3d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hs4cDGosXYp3"
   },
   "source": [
    "#### This method is to only be used when loading a previously partially trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N1u9UGQJXDSC"
   },
   "outputs": [],
   "source": [
    "# model = load_model(\"./drive/My Drive/project-model/saved-model-53.hdf5\")\n",
    "# history = model.fit([x_tr,y_tr[:,:-1]], y_tr, callbacks=[mc], batch_size=BATCH_SIZE, epochs=1, validation_data=([x_val,y_val[:,:-1]], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGgNk6PJFhvi"
   },
   "source": [
    "### Plot training and validation loss overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N52zaYcKlAXE"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "# plt.savefig('loss' + str(build_number) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxSFAxiWlDad"
   },
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLAk2yo6aRh1"
   },
   "outputs": [],
   "source": [
    "# load model from drive\n",
    "# model = load_model(\"./drive/My Drive/saved-model-53.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cm1b46qtlFnT"
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "print(reverse_source_word_index)\n",
    "print(reverse_target_word_index)\n",
    "# used to get positions of sostok and eostok\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1Q7SWH_lGxE"
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "biBjzpLQlPhf"
   },
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAvDUaSWaqYX"
   },
   "outputs": [],
   "source": [
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM*2,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM*2,))\n",
    "decoder_hidden_state_input = Input(shape=(MAX_TEXT_LEN,LATENT_DIM*2))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_inputs)\n",
    "decoder_states = [state_h2, state_c2]\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c], [decoder_outputs2] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmvKd7pNlQXs"
   },
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywir6L6wlUE-"
   },
   "source": [
    "### Methods for Reversing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoKuk0dElZZm"
   },
   "outputs": [],
   "source": [
    "def seq_to_text(input_seq, summary):       \n",
    "  textString=''\n",
    "  if not summary:\n",
    "      for i in input_seq:\n",
    "          if(i!=0):\n",
    "              textString = textString + ' ' + reverse_source_word_index[i]\n",
    "  else:\n",
    "      for i in input_seq:\n",
    "          if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "              textString = textString + ' ' + reverse_target_word_index[i]\n",
    "  return textString "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSR-0MHclb0G"
   },
   "source": [
    "### Summarisation Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cU0yY-1sxsLR"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (MAX_SUMMARY_LEN-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STnQkiZzlhbb"
   },
   "source": [
    "## Test Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7d_RFVVqxfmz"
   },
   "outputs": [],
   "source": [
    "def getRouge(gt, pred):\n",
    "  return rouge.get_scores(pred, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IC1oCOOHlgjh"
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(\"----------------------\")\n",
    "    article = seq_to_text(x_tr[i], False)\n",
    "    original = seq_to_text(y_tr[i], True)\n",
    "\n",
    "    if (original !=\"\"):\n",
    "      print(\"Article: \" + article)\n",
    "      print(\"Original summary:\", original)\n",
    "      # reshape data into correct format for encoder (1, max_text_len)\n",
    "      x_tr_i_reshaped = x_tr[i].reshape(1,MAX_TEXT_LEN)\n",
    "      summary = decode_sequence(x_tr_i_reshaped)\n",
    "      print(\"Generated summary:\",summary)\n",
    "      print(\"\\n\")\n",
    "\n",
    "      if summary != \"\":    \n",
    "          print(\"ROUGE score: \")\n",
    "          score = getRouge(str(summary), str(original))\n",
    "          print(score)\n",
    "          print(score[0].get('rouge-1').get('f'))\n",
    "          print(score[0].get('rouge-1').get('p'))\n",
    "          print(score[0].get('rouge-1').get('r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqxi6N9_zATw"
   },
   "outputs": [],
   "source": [
    "for i in range(1243,1253):\n",
    "    print(\"----------------------\")\n",
    "    article = seq_to_text(x_tr[i], False)\n",
    "    original = seq_to_text(y_tr[i], True)\n",
    "\n",
    "    if (original !=\"\"):\n",
    "      print(\"Article: \" + article)\n",
    "      print(\"Original summary:\", original)\n",
    "      # reshape data into correct format for encoder (1, max_text_len)\n",
    "      x_tr_i_reshaped = x_tr[i].reshape(1,MAX_TEXT_LEN)\n",
    "      summary = decode_sequence(x_tr_i_reshaped)\n",
    "      print(\"Generated summary:\",summary)\n",
    "      print(\"\\n\")\n",
    "\n",
    "      if summary != \"\":    \n",
    "          print(\"ROUGE score: \")\n",
    "          score = getRouge(str(summary), str(original))\n",
    "          print(score)\n",
    "          print(score[0].get('rouge-1').get('f'))\n",
    "          print(score[0].get('rouge-1').get('p'))\n",
    "          print(score[0].get('rouge-1').get('r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNy_KJPb4PXc"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMRIXeuz8hYn"
   },
   "source": [
    "Using ROUGE (Recall-Orientated Understanding Gisting Evaluation) to evaluate the generated summaries.\n",
    "\n",
    "*Note: This takes a long time, especially with large datasets* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwTD6W5kFmvk"
   },
   "source": [
    "### For Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUr2YcMoDGEt"
   },
   "outputs": [],
   "source": [
    "print(len(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BKXHKbaDS8G"
   },
   "outputs": [],
   "source": [
    "tr_target_summary = []\n",
    "tr_generated_summary = []\n",
    "x_tr_len = len(x_tr)\n",
    "\n",
    "f_ov = 0\n",
    "p_ov = 0\n",
    "r_ov = 0\n",
    "# x_val_len = 1\n",
    "for i in range(0,x_tr_len):\n",
    "  print(i)\n",
    "  original = seq_to_text(y_tr[i], True)\n",
    "  if original != \"\" :\n",
    "      tr_target_summary.append(original)\n",
    "      x_i = x_tr[i].reshape(1,MAX_TEXT_LEN)\n",
    "      summary = decode_sequence(x_i)\n",
    "      print(i)\n",
    "      print(original)\n",
    "      print(summary)\n",
    "      print(\"-----\")\n",
    "      tr_generated_summary.append(summary)\n",
    "      score = getRouge(str(summary), str(original))\n",
    "      f_ov += float(score[0].get('rouge-1').get('f'))\n",
    "      p_ov += float(score[0].get('rouge-1').get('p'))\n",
    "      r_ov += float(score[0].get('rouge-1').get('r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuzxQo3E4SlK"
   },
   "outputs": [],
   "source": [
    "print(\"Avg F Score: \" + str(f_ov/x_tr_len))\n",
    "print(\"Avg Precision: \" + str(p_ov/x_tr_len))\n",
    "print(\"Avg Recall: \" + str(r_ov/x_tr_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KiHBkkwmFTPt"
   },
   "source": [
    "### For Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2QGuitHFZD1"
   },
   "outputs": [],
   "source": [
    "val_target_summary = []\n",
    "val_generated_summary = []\n",
    "x_val_len = len(x_val)\n",
    "\n",
    "f_ov = 0\n",
    "p_ov = 0\n",
    "r_ov = 0\n",
    "# x_val_len = 1\n",
    "for i in range(0,x_val_len):\n",
    "  print(i)\n",
    "  original = seq_to_text(y_val[i], True)\n",
    "  if original != \"\" :\n",
    "      val_target_summary.append(original)\n",
    "      x_i = x_val[i].reshape(1,MAX_TEXT_LEN)\n",
    "      summary = decode_sequence(x_i)\n",
    "      print(i)\n",
    "      print(original)\n",
    "      print(summary)\n",
    "      print(\"-----\")\n",
    "      val_generated_summary.append(summary)\n",
    "      score = getRouge(str(summary), str(original))\n",
    "      f_ov += float(score[0].get('rouge-1').get('f'))\n",
    "      p_ov += float(score[0].get('rouge-1').get('p'))\n",
    "      r_ov += float(score[0].get('rouge-1').get('r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9n0nDG-F6KN"
   },
   "outputs": [],
   "source": [
    "print(\"Avg F Score: \" + str(f_ov/x_val_len))\n",
    "print(\"Avg Precision: \" + str(p_ov/x_val_len))\n",
    "print(\"Avg Recall: \" + str(r_ov/x_val_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "faHKW5pUF9KH"
   },
   "source": [
    "# Inputting New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "srBPE2ZxZH72"
   },
   "outputs": [],
   "source": [
    "def getpos(word):\n",
    "  pos = nltk.pos_tag([word])[0][1][0]\n",
    "  wordnet_conv = {\"J\": wn.ADJ, \"N\": wn.NOUN, \"V\": wn.VERB, \"R\": wn.ADV}\n",
    "  if pos in wordnet_conv.keys():\n",
    "    return wordnet_conv.get(pos)\n",
    "  return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5aEv4W8ZM81"
   },
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  text_tokenized = inp_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "  print(\"lemmatize with pos\")\n",
    "  for i in range(0,len(text_tokenized)):\n",
    "    text_lemmatized = []\n",
    "    for word in text_tokenized[i]:\n",
    "      pos = getpos(word)\n",
    "      if pos != \"\":\n",
    "        lemma = lemmatizer.lemmatize(word, pos)\n",
    "        text_lemmatized.append(lemma)\n",
    "      else :\n",
    "        text_lemmatized.append(word)\n",
    "    text_lemmatized = ' '.join(map(str, text_lemmatized))\n",
    "    inp_df['text'][i] = text_lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5tlZ-4XXuctn"
   },
   "outputs": [],
   "source": [
    "input1 = \"(CNN) — Earlier this year, Delta Air Lines announced a rethink on reclining seats. In an effort to disrupt fewer passengers' travel experiences, Delta said it'd begin revamping some of its jets to reduce the recline of coach seats from four inches to two inches and the recline of first class seats from 5.5 inches to 3.5 inches. For those who abhor the recline option, it's a small step. And for those who value it, well, it's a compromise. This seemingly innocuous topic is one where there are very much two minds on what's acceptable and what's not. Two CNN Travel staffers engage in a friendly debate about seat recline. Your seat. Your decision. Stacey Lastoe, senior editor at CNN Travel, is of above-average height and makes no apology about reclining; it's her right as a plane, train and bus passenger. She encourages the person sitting in front of her to recline as well. On the first leg of my flight to Japan for my honeymoon, my husband and I got upgraded to first class. Although it would just be a few hours in the sky en route to Dallas, I was excited about sipping Champagne, sitting back and relaxing. Flute in hand, I pushed back to recline my seat for maximum relaxation. But it would not budge; I appeared to be stuck in a dysfunctional seat. Or was I? Turns out the gentleman behind me had a dog in a crate down between his legs, positioned so the seat in front of his -- my seat -- had nowhere to go. Because we were newlyweds and loving every moment of it, I did not mind when my husband turned to the man and told him his wife wanted to recline her seat and asked if he could please rearrange his dog crate to allow for everyones comfort.\"\n",
    "# input1 = \"Its official US President Barack Obama want lawmaker weigh whether use military force Syria Obama sent letter head House Senate Saturday night hour announce believe military action Syrian target right step take allege use chemical weapon The propose legislation Obama asks Congress approve use military force deter disrupt prevent degrade potential future us chemical weapon weapon mass destruction Its step set turn international crisis fierce domestic political battle There key question loom debate What UN weapon inspector find Syria What happens Congress vote And Syrian government react In televise address White House Rose Garden earlier Saturday president say would take case Congress want While I believe I authority carry military action without specific congressional authorization I know country strong take course action even effective say We debate issue big business usual Obama say top congressional leader agree schedule debate body return Washington September 9 The Senate Foreign Relations Committee hold hearing matter Tuesday Sen Robert Menendez say Transcript Read Obamas full remark Syrian crisis Latest development UN inspector leave Syria Obamas remark come shortly UN inspector left Syria carry evidence determine whether chemical weapon use attack early last week Damascus suburb The aim game mandate clear ascertain whether chemical weapon use UN spokesman Martin Nesirky told reporter Saturday But use weapon report toxic gas attack Damascus suburb August 21 key point global debate Syrian crisis Top US official say there doubt Syrian government behind Syrian official deny responsibility blame jihadist fight rebel British US intelligence report say attack involve chemical weapon UN official stress importance wait official report inspector The inspector share finding UN SecretaryGeneral Ban Kimoon Ban say want wait UN team final report complete present UN Security Council The Organization Prohibition Chemical Weapons nine inspector belong say Saturday could take three week analyze evidence collect It need time able analyze information sample Nesirky say He note Ban repeatedly say alternative political solution crisis Syria military solution option Bergen Syria problem hell US Obama This menace must confront Obamas senior adviser debate next step take president comment Saturday come amid mount political pressure situation Syria Some US lawmaker call immediate action others warn step could become quagmire Some global leader express support British Parliaments vote military action earlier week blow Obamas hope get strong backing key NATO ally On Saturday Obama propose say would limited military action Syrian President Bashar alAssad Any military attack would openended include US ground force say Syrias allege use chemical weapon earlier month assault human dignity president say A failure respond force Obama argue could lead escalate use chemical weapon proliferation terrorist group would people harm In world many danger menace must confront Syria missile strike What would happen next Map US allied asset around Syria Obama decision come Friday night On Friday night president make lastminute decision consult lawmaker What happen vote Its unclear A senior administration official told CNN Obama authority act without Congress even Congress reject request authorization use force Obama Saturday continued shore support strike alAssad government He spoke phone French President Francois Hollande Rose Garden speech The two leader agree international community must deliver resolute message Assad regime others would consider use chemical weapon crime unacceptable violate international norm held accountable world White House say Meanwhile uncertainty loom Congress would weigh US military official say remain ready 5 key assertion US intelligence report Syria Syria Who want chemical weapon horror Reactions mixed Obamas speech A spokesman Syrian National Coalition say opposition group disappointed Obamas announcement Our fear lack action could embolden regime repeat attack serious way say spokesman Louay Safi So quite concerned Some member Congress applaud Obamas decision House Speaker John Boehner Majority Leader Eric Cantor Majority Whip Kevin McCarthy Conference Chair Cathy McMorris Rodgers issue statement Saturday praise president Under Constitution responsibility declare war lie Congress Republican lawmaker say We glad president seek authorization military action Syria response serious substantive question raise More 160 legislator include 63 Obamas fellow Democrats sign letter call either vote least full debate US action British Prime Minister David Cameron whose attempt get lawmaker country support military action Syria fail earlier week respond Obamas speech Twitter post Saturday I understand support Barack Obamas position Syria Cameron say An influential lawmaker Russia stood Syria criticize United States theory The main reason Obama turn Congress military operation get enough support either world among ally US United States Alexei Pushkov chairman internationalaffairs committee Russian State Duma say Twitter post In United States scatter group antiwar protester around country take street Saturday Like many Americanswere tire United States get involve invade bombing country say Robin Rosecrans among hundred Los Angeles demonstration What Syrias neighbor think Why Russia China Iran stand Assad Syrias government unfazed After Obamas speech military political analyst Syrian state TV say Obama embarrass Russia opposes military action Syria cry help someone come rescue face two defeat political military level Syrias prime minister appear unfazed saberrattling The Syrian Armys status maximum readiness finger trigger confront challenge Wael Nader alHalqi say meeting delegation Syrian expatriate Italy accord banner Syria State TV broadcast prior Obamas address An anchor Syrian state television say Obama appear prepare aggression Syria base repeat lie A top Syrian diplomat told state television network Obama face pressure take military action Israel Turkey Arabs rightwing extremist United States I think do well Cameron term take issue Parliament say Bashar Jaafari Syrias ambassador United Nations Both Obama Cameron say climbed top tree dont know get The Syrian government deny use chemical weapon August 21 attack say jihadist fight rebel use effort turn global sentiment British intelligence put number people kill attack 350 On Saturday Obama say told well 1000 people murder US Secretary State John Kerry Friday cite death toll 1429 400 child No explanation offer discrepancy Iran US military action Syria would spark disaster Opinion Why strike Syria bad idea\"\n",
    "inp_df = pd.DataFrame(columns=['text', 'summary'])\n",
    "inp_df = inp_df.append({'text': str(input1), 'summary': \"\"}, ignore_index=True)\n",
    "inp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OyDVNd3pvxIX"
   },
   "outputs": [],
   "source": [
    "inp_df['text'] = inp_df['text'].apply(lambda x: re.sub(r'\\(CNN\\)|--|[^\\w\\s\\.]','',x)).apply(lambda x: re.sub(r'(\\.(?=[\\s\\r\\n]|$))','',x)).apply(lambda x: re.sub(r'\\n',' ',x)).apply(lambda x: re.sub(r'\\.','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WyRVL-DGACF"
   },
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "inp_df['text'] = inp_df['text'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: \" \".join([word for word in x if not word in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nd2C6wFAxU3K"
   },
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "lemmatization(inp_df['text'])\n",
    "print(inp_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwj2UIuQ20Hl"
   },
   "outputs": [],
   "source": [
    "seq = np.array(inp_df['text'])\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xx7fUFkZ0aAr"
   },
   "outputs": [],
   "source": [
    "seq_tokenizer = x_tokenizer.texts_to_sequences(seq)\n",
    "#padding zero upto maximum length\n",
    "seq_tokenizer_padded = pad_sequences(seq_tokenizer,  maxlen=MAX_TEXT_LEN, padding='post')\n",
    "gen_summary = decode_sequence(seq_tokenizer_padded)\n",
    "original_txt = ' '.join(seq)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Original: \" + original_txt)\n",
    "print(\"Generated Summary: \" + gen_summary)\n",
    "# this will probs fail\n",
    "print(\"ROUGE score: \")\n",
    "print(getRouge(summary, original_txt))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "final_bi_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
