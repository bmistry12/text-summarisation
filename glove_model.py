# -*- coding: utf-8 -*-
"""glove-model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tEIHOv6QCKnQ1itrnr7T5yxi-KJEgSvd
"""

import re
import nltk
import pickle
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
!python -m nltk.downloader stopwords wordnet punkt averaged_perceptron_tagger
from nltk.corpus import stopwords
from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer
from keras.preprocessing.text import Tokenizer 
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional
from tensorflow.keras.layers import Attention
from keras.models import Model, load_model
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.text import Tokenizer 
from keras.preprocessing.sequence import pad_sequences
from keras.optimizers import RMSprop
from sklearn.model_selection import train_test_split
!pip install rouge
from rouge import Rouge
from sklearn.model_selection import StratifiedKFold

"""### Hyperparameters"""

BATCH_SIZE=10
EPOCHS=200
LATENT_DIM=256
# choice of 50, 100, 200, 300
EMBEDDING_DIM=50
TEST_TRAIN_SPLIT=0.15
LEARNING_RATE=0.0001
PATH=""
FILE_NAME="originals-l.csv"
MAX_TEXT_LEN = 20
MAX_SUMMARY_LEN = 10
# system booleans
COLAB=True # true if running on colab
build_number="1"

# create rouge object for evaluation
rouge = Rouge()


"""## Data Processing

Read In Data
"""
if COLAB:
  from google.colab import drive
  drive.mount('/content/drive')
  PATH = "./drive/My Drive/"

df.pd_read_csv(PATH + FILE_NAME)

print(df.head())
print(df.count)

"""Remove .'s that appear in stuff like U.S.A and U.N - Eventually need to move this to dataprocessing.py"""

print(df['summary'][0])
df['summary'] = df['summary'].apply(lambda x: re.sub(r'\..*$',' ',str(x)))
print(df['summary'][0])

print(df['summary'][0])
df['summary'] = df['summary'].apply(lambda x: re.sub(r'\.','',str(x)))
print(df['summary'][0])

"""Check for rows with null values in them, and copy these into a new dataframe (df1). Drop any rows in df1 from df to ensure no NaN valued rows are present/

*Note. using simply dropna(how='any') does not seem to drop any of the rows*
"""

print(df.isnull().values.any())
print(df.shape)

df1 = df[df.isna().any(axis=1)]
print(df1.shape)

df.drop(df1.index, axis=0,inplace=True)
print(df.shape)
print(df.isnull().values.any())

"""Cut down text to MAX_TEXT_LEN words, and summaries to MAX_SUMMARY_LEN"""

print(df['text'][0])
df['text'] = df['text'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: " ".join(x[:MAX_TEXT_LEN]))
print(df['text'][0])

print(df['summary'][0])
df['summary'] = df['summary'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: " ".join(x[:MAX_SUMMARY_LEN]))
print(df['summary'][0])

"""Word Count Distribution"""

text_word_count = []
summary_word_count = []

# populate the lists with sentence lengths
for i in df['text']:
      text_word_count.append(len(i.split(' ')))

for i in df['summary']:
      summary_word_count.append(len(i.split(' ')))

length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})

length_df.hist(bins = 30)
plt.ylabel('Documents')
plt.xlabel('Word Count')
plt.savefig('word_count_distro_glove' + str(build_number) + '.png')

# Finding common words and removing them
word_dict = {}
text = df['text'].apply(lambda x: nltk.word_tokenize(x))

for index, row in text.iteritems():
  for word in row:
    if word not in word_dict.keys():
      word_dict[word] = 1
    else:
      word_dict[word] += 1

print(len(word_dict))
sorted_dict = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)
print(sorted_dict)
x, y = zip(*sorted_dict)

accept_words = list(x[3:])
accept_words = [x.lower() for x in accept_words]
print(accept_words)
print(df['text'][2])
df['text'] = df['text'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: " ".join([word for word in x if word.lower() in accept_words]))
print(df['text'][2])

word_dict = {}
text = df['summary'].apply(lambda x: nltk.word_tokenize(x))

for index, row in text.iteritems():
  for word in row:
    if word not in word_dict.keys():
      word_dict[word] = 1
    else:
      word_dict[word] += 1

print(len(word_dict))
sorted_dict = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)
print(sorted_dict)
x, y = zip(*sorted_dict)

accept_words = list(x[3:])
accept_words = [x.lower() for x in accept_words]
print(accept_words)
print(df['summary'][2])
df['summary'] = df['summary'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: " ".join([word for word in x if word.lower() in accept_words]))
print(df['summary'][2])

text_word_count = []
summary_word_count = []

# populate the lists with sentence lengths
for i in df['text']:
      text_word_count.append(len(i.split(' ')))

for i in df['summary']:
      summary_word_count.append(len(i.split(' ')))

length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})

length_df.hist(bins = 30)
plt.ylabel('Documents')
plt.xlabel('Word Count')
plt.savefig('word_count_distro_removed_glove' + str(build_number) + '.png')

"""Max Text Lengths"""

MAX_TEXT_LEN = max([len(txt.split(' ')) for txt in df['text']])
MAX_SUMMARY_LEN = max([len(txt.split(' ')) for txt in df['summary']])
print(MAX_TEXT_LEN)
print(MAX_SUMMARY_LEN)

"""### Training-Validation Split

X - Articles text </br>
Y - Summaries
"""

# convert to numpy array
X = np.array(df['text'])
Y = np.array(df['summary'])

x_tr,x_val,y_tr,y_val=train_test_split(X,Y,test_size=TEST_TRAIN_SPLIT,random_state=0,shuffle=True)
print(x_tr.shape)
print(x_val.shape)
print(y_tr.shape)
print(y_val.shape)

"""## GloVe"""

embedding_index = {}
with open('./drive/My Drive/glove/glove.6B.' + str(EMBEDDING_DIM) + 'd.txt') as f:
  for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embedding_index[word] = coefs

print(len(embedding_index))

print(embedding_index.get("us"))

"""### Word Embeddings - Tokenization

X Tokenizer
"""

word_dict = {}
text = df['text']

for row in text: 
  for word in row.split(" "):
    if word not in word_dict:
      word_dict[word] = 1
    else:
      word_dict[word] += 1

print(len(word_dict))

#prepare a tokenizer for reviews on training data
x_tokenizer = Tokenizer(num_words=len(word_dict), split=" ") 
x_tokenizer.fit_on_texts(list(X))

x_embedding_matrix = np.zeros(((x_tokenizer.num_words)+1, EMBEDDING_DIM),dtype='float32')
print(x_embedding_matrix.shape)
for word,i in x_tokenizer.word_index.items():
    embedding_vector = embedding_index.get(word)
    if embedding_vector is not None:
        # Words not found in glove will be zeros
        x_embedding_matrix[i] = embedding_vector

# #convert text sequences into integer sequences
x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) 
x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)

#size of vocabulary ( +1 for padding token)
x_voc   =  x_tokenizer.num_words + 1

#padding zero upto maximum length
x_tr    =   pad_sequences(x_tr_seq,  maxlen=x_voc, padding='post')
x_val   =   pad_sequences(x_val_seq, maxlen=x_voc, padding='post')

print(x_tokenizer.num_words)
print(x_voc)

"""Y Tokenizer"""

y_word_dict = {}
summ = df['summary']

for row in summ: 
  for word in row.split(" "):
    if word not in y_word_dict:
      y_word_dict[word] = 1
    else:
      y_word_dict[word] += 1

print(len(y_word_dict))

#prepare a tokenizer for reviews on training data
y_tokenizer = Tokenizer(num_words=len(y_word_dict), split=" ") 
y_tokenizer.fit_on_texts(list(Y))

y_embedding_matrix = np.zeros((x_tokenizer.num_words +1, EMBEDDING_DIM),dtype='float32')
print(y_embedding_matrix.shape)
print(len( y_tokenizer.word_index))
for word,i in y_tokenizer.word_index.items():
    embedding_vector = embedding_index.get(word)
    if embedding_vector is not None:
    # Words not found in glove will be zeros
        y_embedding_matrix[i] = embedding_vector

#convert text sequences into integer sequences
y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) 
y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) 

#size of vocabulary
y_voc  =   y_tokenizer.num_words + 1

#padding zero upto maximum length
y_tr    =   pad_sequences(y_tr_seq, maxlen=x_voc, padding='post')
y_val   =   pad_sequences(y_val_seq, maxlen=x_voc, padding='post')

print(y_voc)

"""### GloVe Word Coverage"""

def coverage(dict, total):
  covered = 0
  for words, _ in dict.items():
    if embedding_index.get(word) is not None:
      covered += 1
  return (covered/total * 100)

text_total = len()
text_covered = coverage(x_word_dict,text_total)

summ_total = len()
summ_covered = coverage(y_word_dict,summ_total)

print("Original Text Coverage: " + str(text_covered) + "%")
print("Summary Coverage: " + str(summ_covered) + "%")

"""## Learning Model

#### Encoder Model
"""

# bidirectional encoder
encoder_inputs = Input(shape=(x_voc,))
# embedding layer
enc_emb_layer = Embedding(x_voc, EMBEDDING_DIM, weights=[x_embedding_matrix], 
                          input_length=x_voc, trainable=False)
enc_emb = enc_emb_layer(encoder_inputs)

#encoder lstm 
encoder_lstm = Bidirectional(LSTM(LATENT_DIM,return_sequences=True,return_state=True))
encoder_outputs, fw_state_h, fw_state_c, bw_state_h, bw_state_c = encoder_lstm(enc_emb)

state_h = Concatenate()([fw_state_h, bw_state_h])
state_c = Concatenate()([fw_state_c, bw_state_c])
encoder_states = [state_h, state_c]

"""#### Decoder Model"""

# Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = Input(shape=(x_voc,))

#embedding layer
dec_emb_layer = Embedding(x_voc, EMBEDDING_DIM,
                          weights=[y_embedding_matrix], input_length=x_voc, 
                          trainable=False)
dec_emb = dec_emb_layer(decoder_inputs)

decoder_lstm = LSTM(LATENT_DIM*2, return_sequences=True, return_state=True)
decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(
    dec_emb, initial_state=encoder_states)
                                                          
#dense layer
decoder_dense = Dense(y_voc, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

"""#### Combined LSTM Model"""

# Define the model 
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

print(model.summary())

optimizer = RMSprop(lr=LEARNING_RATE, rho=0.9)

model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')

"""- Early Stopping Callback to ensure we stop when Validation Loss is lowest - minimises risk of overfitting
- Model Checkpoint saves the model after each epoch so that we can load the model with the best weights later on. Alternatively, it allows us to continue training the model at a later data
"""

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3, restore_best_weights=False)
filepath = "./drive/My Drive/project-model/saved-model-{epoch:02d}.hdf5"
mc = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)

"""#### Use this method to train a new model. To continue training a previously trained model see below"""

print(x_tr.shape)
print(y_tr.shape)
print(y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1).shape)
print(y_val.reshape(y_val.shape[0],y_val.shape[1], 1).shape)
y_tr_3d = y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)
y_val_3d = y_val.reshape(y_val.shape[0],y_val.shape[1], 1)

history = model.fit([x_tr,y_tr], y_tr_3d, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[es],
                    validation_data=([x_val,y_val], y_val_3d))

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend()
plt.savefig('loss_glove_' + str(build_number) + '.png')

reverse_target_word_index=y_tokenizer.index_word
reverse_source_word_index=x_tokenizer.index_word
print(reverse_source_word_index)
print(reverse_target_word_index)

# Encode the input sequence to get the feature vector
encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])

# encoder_model.summary()

# Decoder setup
# Below tensors will hold the states of the previous time step
decoder_state_input_h = Input(shape=(LATENT_DIM*2,))
decoder_state_input_c = Input(shape=(LATENT_DIM*2,))
decoder_hidden_state_input = Input(shape=(x_voc,LATENT_DIM*2))
decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]

# Get the embeddings of the decoder sequence
dec_emb2= dec_emb_layer(decoder_inputs) 
# To predict the next word in the sequence, set the initial states to the states from the previous time step
decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_inputs)
decoder_states = [state_h2, state_c2]
# A dense softmax layer to generate prob dist. over the target vocabulary
decoder_outputs2 = decoder_dense(decoder_outputs2) 

# Final decoder model
decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c], [decoder_outputs2] + decoder_states)
# decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs2] + decoder_states)

# decoder_model.summary()

"""### Methods for Reversing Word Embeddings"""

def seq2summary(input_seq):
    newString=''
    for i in input_seq:
        if(i!=0):
            newString=newString+reverse_target_word_index[i]+' '
    return newString

def seq2text(input_seq):
    newString=''
    for i in input_seq:
        if(i!=0):
            newString=newString+reverse_source_word_index[i]+' '
    return newString

"""### Summarisation Method"""

def decode_sequence(input_seq): 
    # Encode the input as state vectors.
    print(input_seq)
    print(input_seq.shape)
    e_out, e_h, e_c = encoder_model.predict(input_seq)
    print(e_out)
    # target = [0,0,0....n] where n = y_voc
    target_seq = np.zeros((1, x_voc))
    stop_condition = False
    decoded_sentence = ''

    while not stop_condition:
      # Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs2] + decoder_states)
      # print(target_seq)
      output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c]) 
      print(output_tokens.shape)
      print(output_tokens)
      print("output tokens")
      print(output_tokens[0,-1,:])
      print("output tokens")
      print(output_tokens[0,-1,:][1:])
      sampled_token_index = np.argmax(output_tokens[0, -1, :][1:])
      print(sampled_token_index)
      if (sampled_token_index != 0 ):
        sampled_token = reverse_target_word_index[sampled_token_index]
        # print(sampled_token)
        decoded_sentence += ' '+sampled_token
      else :
        print("sadface")
        stop_condition = True
      if (len(decoded_sentence.split()) >= (MAX_SUMMARY_LEN-1)):
              stop_condition = True
       # Update the target sequence (of length 1).
      # target_seq = np.zeros((1, x_voc))
      target_seq[0, sampled_token_index] = 1

      # Update internal states
      e_h, e_c = h, c
      
    return decoded_sentence

"""## Test Model Output"""

def getRouge(gt, pred):
  return rouge.get_scores(pred, gt)

for i in range(0,5):
    print("Article:",seq2text(x_tr[i]))
    original = seq2summary(y_tr[i])
    print("Original summary:",original)
    x_tr_i_reshaped = x_tr[i].reshape(1,x_voc)
    summary = decode_sequence(x_tr_i_reshaped)
    print("Generated summary:",summary)
    print("\n")

    if summary != "":    
      print("ROUGE score: ")
      score = getRouge(str(summary), str(original))
      print(score)
      print(score[0].get('rouge-1').get('f'))
      print(score[0].get('rouge-1').get('p'))
      print(score[0].get('rouge-1').get('r'))


"""## Evaluation

Using ROUGE (Recall-Orientated Understanding Gisting Evaluation) to evaluate the generated summaries.

*Note: This takes a long time, especially with large datasets*
"""

# def get_overlapping_words(x, y):
#   num=0
#   x = nltk.word_tokenize(x)
#   y = nltk.word_tokenize(y)
#   for word in y:
#     if word in x:
#       num = num+1
#       x.remove(word)
#     else:
#       return num

# def precision(target, generated):
#   length = len(target)
#   for i in range (0, length):
#     num_overlapping_words = get_overlapping_words(target[i], generated[i])
#     generated_summary_len = len(generated[i])
#     if generated_summary_len == 0 :
#         return 0.0
#     else : 
#       return num_overlapping_words / generated_summary_len

"""### For Training Data"""

print(len(x_tr))

tr_target_summary = []
tr_generated_summary = []
x_tr_len = len(x_tr)

f_ov = 0
p_ov = 0
r_ov = 0
# x_val_len = 1
for i in range(0,x_tr_len):
  original = seq2summary(y_tr[i])
  tr_target_summary.append(original)
  x_i = x_tr[i].reshape(1,x_voc)
  summary = decode_sequence(x_i)
  tr_generated_summary.append(summary)
  score = getRouge(str(summary), str(original))
  f_ov += float(score[0].get('rouge-1').get('f'))
  p_ov += float(score[0].get('rouge-1').get('p'))
  r_ov += float(score[0].get('rouge-1').get('r'))

# print("precision : " + str(precision(tr_target_summary, tr_generated_summary)))
print("Avg F Score: " + str(f_ov/x_tr_len))
print("Avg Precision: " + str(p_ov/x_tr_len))
print("Avg Recall: " + str(r_ov/x_tr_len))

"""### For Validation Data"""

val_target_summary = []
val_generated_summary = []
x_val_len = len(x_val)
f_ov = 0
p_ov = 0
r_ov = 0

for i in range(0,x_val_len):
  original = seq2summary(y_val[i])
  val_target_summary.append(original)
  x_i = x_val[i].reshape(1,x_voc)
  summary = decode_sequence(x_i)
  val_generated_summary.append(summary)
  score = getRouge(str(summary), str(original))
  f_ov += float(score[0].get('rouge-1').get('f'))
  p_ov += float(score[0].get('rouge-1').get('p'))
  r_ov += float(score[0].get('rouge-1').get('r'))

# print("precision : " + str(precision(val_target_summary, val_generated_summary)))
print("Avg F Score: " + str(f_ov/x_val_len))
print("Avg Precision: " + str(p_ov/x_val_len))
print("Avg Recall: " + str(r_ov/x_val_len))

# """# Inputting New Data"""

# def getpos(word):
#   pos = nltk.pos_tag([word])[0][1][0]
#   wordnet_conv = {"J": wn.ADJ, "N": wn.NOUN, "V": wn.VERB, "R": wn.ADV}
#   if pos in wordnet_conv.keys():
#     return wordnet_conv.get(pos)
#   return ""

# def lemmatization(text):
#   lemmatizer = WordNetLemmatizer()
#   text_tokenized = inp_df['text'].apply(lambda x: nltk.word_tokenize(x))
#   print("lemmatize with pos")
#   for i in range(0,len(text_tokenized)):
#     text_lemmatized = []
#     for word in text_tokenized[i]:
#       pos = getpos(word)
#       if pos != "":
#         lemma = lemmatizer.lemmatize(word, pos)
#         text_lemmatized.append(lemma)
#       else :
#         text_lemmatized.append(word)
#     text_lemmatized = ' '.join(map(str, text_lemmatized))
#     inp_df['text'][i] = text_lemmatized

# # input1 = "(CNN) â€” Earlier this year, Delta Air Lines announced a rethink on reclining seats. In an effort to disrupt fewer passengers' travel experiences, Delta said it'd begin revamping some of its jets to reduce the recline of coach seats from four inches to two inches and the recline of first class seats from 5.5 inches to 3.5 inches. For those who abhor the recline option, it's a small step. And for those who value it, well, it's a compromise. This seemingly innocuous topic is one where there are very much two minds on what's acceptable and what's not. Two CNN Travel staffers engage in a friendly debate about seat recline. Your seat. Your decision. Stacey Lastoe, senior editor at CNN Travel, is of above-average height and makes no apology about reclining; it's her right as a plane, train and bus passenger. She encourages the person sitting in front of her to recline as well. On the first leg of my flight to Japan for my honeymoon, my husband and I got upgraded to first class. Although it would just be a few hours in the sky en route to Dallas, I was excited about sipping Champagne, sitting back and relaxing. Flute in hand, I pushed back to recline my seat for maximum relaxation. But it would not budge; I appeared to be stuck in a dysfunctional seat. Or was I? Turns out the gentleman behind me had a dog in a crate down between his legs, positioned so the seat in front of his -- my seat -- had nowhere to go. Because we were newlyweds and loving every moment of it, I did not mind when my husband turned to the man and told him his wife wanted to recline her seat and asked if he could please rearrange his dog crate to allow for everyones comfort."
# input1 = "Its official US President Barack Obama want lawmaker weigh whether use military force Syria Obama sent letter head House Senate Saturday night hour announce believe military action Syrian target right step take allege use chemical weapon The propose legislation Obama asks Congress approve use military force deter disrupt prevent degrade potential future us chemical weapon weapon mass destruction Its step set turn international crisis fierce domestic political battle There key question loom debate What UN weapon inspector find Syria What happens Congress vote And Syrian government react In televise address White House Rose Garden earlier Saturday president say would take case Congress want While I believe I authority carry military action without specific congressional authorization I know country strong take course action even effective say We debate issue big business usual Obama say top congressional leader agree schedule debate body return Washington September 9 The Senate Foreign Relations Committee hold hearing matter Tuesday Sen Robert Menendez say Transcript Read Obamas full remark Syrian crisis Latest development UN inspector leave Syria Obamas remark come shortly UN inspector left Syria carry evidence determine whether chemical weapon use attack early last week Damascus suburb The aim game mandate clear ascertain whether chemical weapon use UN spokesman Martin Nesirky told reporter Saturday But use weapon report toxic gas attack Damascus suburb August 21 key point global debate Syrian crisis Top US official say there doubt Syrian government behind Syrian official deny responsibility blame jihadist fight rebel British US intelligence report say attack involve chemical weapon UN official stress importance wait official report inspector The inspector share finding UN SecretaryGeneral Ban Kimoon Ban say want wait UN team final report complete present UN Security Council The Organization Prohibition Chemical Weapons nine inspector belong say Saturday could take three week analyze evidence collect It need time able analyze information sample Nesirky say He note Ban repeatedly say alternative political solution crisis Syria military solution option Bergen Syria problem hell US Obama This menace must confront Obamas senior adviser debate next step take president comment Saturday come amid mount political pressure situation Syria Some US lawmaker call immediate action others warn step could become quagmire Some global leader express support British Parliaments vote military action earlier week blow Obamas hope get strong backing key NATO ally On Saturday Obama propose say would limited military action Syrian President Bashar alAssad Any military attack would openended include US ground force say Syrias allege use chemical weapon earlier month assault human dignity president say A failure respond force Obama argue could lead escalate use chemical weapon proliferation terrorist group would people harm In world many danger menace must confront Syria missile strike What would happen next Map US allied asset around Syria Obama decision come Friday night On Friday night president make lastminute decision consult lawmaker What happen vote Its unclear A senior administration official told CNN Obama authority act without Congress even Congress reject request authorization use force Obama Saturday continued shore support strike alAssad government He spoke phone French President Francois Hollande Rose Garden speech The two leader agree international community must deliver resolute message Assad regime others would consider use chemical weapon crime unacceptable violate international norm held accountable world White House say Meanwhile uncertainty loom Congress would weigh US military official say remain ready 5 key assertion US intelligence report Syria Syria Who want chemical weapon horror Reactions mixed Obamas speech A spokesman Syrian National Coalition say opposition group disappointed Obamas announcement Our fear lack action could embolden regime repeat attack serious way say spokesman Louay Safi So quite concerned Some member Congress applaud Obamas decision House Speaker John Boehner Majority Leader Eric Cantor Majority Whip Kevin McCarthy Conference Chair Cathy McMorris Rodgers issue statement Saturday praise president Under Constitution responsibility declare war lie Congress Republican lawmaker say We glad president seek authorization military action Syria response serious substantive question raise More 160 legislator include 63 Obamas fellow Democrats sign letter call either vote least full debate US action British Prime Minister David Cameron whose attempt get lawmaker country support military action Syria fail earlier week respond Obamas speech Twitter post Saturday I understand support Barack Obamas position Syria Cameron say An influential lawmaker Russia stood Syria criticize United States theory The main reason Obama turn Congress military operation get enough support either world among ally US United States Alexei Pushkov chairman internationalaffairs committee Russian State Duma say Twitter post In United States scatter group antiwar protester around country take street Saturday Like many Americanswere tire United States get involve invade bombing country say Robin Rosecrans among hundred Los Angeles demonstration What Syrias neighbor think Why Russia China Iran stand Assad Syrias government unfazed After Obamas speech military political analyst Syrian state TV say Obama embarrass Russia opposes military action Syria cry help someone come rescue face two defeat political military level Syrias prime minister appear unfazed saberrattling The Syrian Armys status maximum readiness finger trigger confront challenge Wael Nader alHalqi say meeting delegation Syrian expatriate Italy accord banner Syria State TV broadcast prior Obamas address An anchor Syrian state television say Obama appear prepare aggression Syria base repeat lie A top Syrian diplomat told state television network Obama face pressure take military action Israel Turkey Arabs rightwing extremist United States I think do well Cameron term take issue Parliament say Bashar Jaafari Syrias ambassador United Nations Both Obama Cameron say climbed top tree dont know get The Syrian government deny use chemical weapon August 21 attack say jihadist fight rebel use effort turn global sentiment British intelligence put number people kill attack 350 On Saturday Obama say told well 1000 people murder US Secretary State John Kerry Friday cite death toll 1429 400 child No explanation offer discrepancy Iran US military action Syria would spark disaster Opinion Why strike Syria bad idea"
# inp_df = pd.DataFrame(columns=['text', 'summary'])
# inp_df = inp_df.append({'text': str(input1), 'summary': ""}, ignore_index=True)
# inp_df.head()

# inp_df['text'] = inp_df['text'].apply(lambda x: re.sub(r'\(CNN\)|--|[^\w\s\.]','',x)).apply(lambda x: re.sub(r'(\.(?=[\s\r\n]|$))','',x)).apply(lambda x: re.sub(r'\n',' ',x)).apply(lambda x: re.sub(r'\.','',x))

# # remove stop words
# stop_words = set(stopwords.words('english'))
# inp_df['text'] = inp_df['text'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: " ".join([word for word in x if not word in stop_words]))

# #lemmatize
# lemmatization(inp_df['text'])
# print(inp_df['text'])

# seq = np.array(inp_df['text'])
# print(seq)

# seq_tokenizer = x_tokenizer.texts_to_sequences(seq)
# #padding zero upto maximum length
# seq_tokenizer_padded = pad_sequences(seq_tokenizer,  maxlen=MAX_TEXT_LEN, padding='post')

# gen_summary = decode_sequence(seq_tokenizer_padded)

# original_txt = ' '.join(seq)

# print("---")
# print("Original: " + original_txt)
# print("Generated Summary: " + gen_summary)
# print("ROUGE score: ")
# print(getRouge(summary, original_txt))